[Unit]
Description=Spark Worker
#After=spark.master@.service

[Service]
TimeoutStartSec=0
ExecStartPre=-/usr/bin/docker kill spark-worker%i
ExecStartPre=-/usr/bin/docker rm spark-worker%i
ExecStartPre=/usr/bin/docker pull luckysahaf/spark-worker:1.1.0
EnvironmentFile=/etc/environment
ExecStart=/usr/bin/bash -c '\
    sudo chmod +x /etc/environment ;\
    . /etc/environment ;\
    alias=$(echo %i | awk -F "-" {\'print$1\'}) ;\
    master=spark-master$alias ;\
    worker=spark-worker%i ;\
    worker_dir=/home/core/run/$worker ;\
    rm -r $worker_dir ;\
    mkdir -p $worker_dir ;\
    to_publish=$(etcdctl get /etcd_spark/$master/$worker/to_publish) ;\
    datanode_port=$(etcdctl get /etcd_spark/$master/$worker/DATANODE_PORT) ;\
    etcdctl get /etcd_spark/$master/$worker/spark_env > $worker_dir/spark-env.sh ;\
    worker_ui=$(etcdctl get /etcd_spark/$master/$worker/WORKER_UI) ;\
    worker_port=$(etcdctl get /etcd_spark/$master/$worker/WORKER_PORT) ;\
    etcdctl get /etcd_spark/$master/log4j > $worker_dir/log4j.properties ;\
    echo "export SPARK_WORKER_PORT=$worker_port" >> $worker_dir/spark-env.sh ;\
    publish_args=$( echo ${COREOS_PRIVATE_IPV4} $to_publish | awk \'{\
        split($0,arr," ");\
        for ( i=2; i <= NF; i++ ) {\
            if ( i == 2 ) { \
                print "-p " arr[i] ":" arr[i];\
            } else { \
                print "-p " arr[1] ":" arr[i] ":" arr[i];\
            } \
        }\
    }\' ) ;\
env_args="-e ETCD_ADDRESS=${COREOS_PRIVATE_IPV4} -e ETCD_PORT=4001 -e HOST_ADDRESS=${COREOS_PRIVATE_IPV4} -e SPARK_WORKER_UI_PORT=$worker_ui -e DATANODE_PORT=$datanode_port" ;\
docker run --name $worker -h $worker $publish_args $env_args -v $worker_dir:/home/run luckysahaf/spark-worker:1.1.0 $master'
ExecStop=/usr/bin/docker stop spark-worker%i

[X-Fleet]
Conflicts=spark.master@*.service
